{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55235c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchtext.data import Field, BucketIterator, TabularDataset\n",
    "from pytorch_pretrained import BertModel, BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f543473",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a6cd52f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_path = 'BERT_BASE'\n",
    "tokenizer = BertTokenizer.from_pretrained(bert_path)\n",
    "vocab_size = 30522\n",
    "bert_emb_dim = 768\n",
    "max_input_length = 512\n",
    "bert = BertModel.from_pretrained(bert_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "466f1e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_cut(sentence):\n",
    "    tokens = tokenizer.tokenize(sentence)\n",
    "    tokens = tokens[:max_input_length-2]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "58ed4ecc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['what', 'the', 'fuck', 'are', 'you', 'talking', 'about', '?']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize_and_cut('What the fuck are you talking about?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e243c761",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2054, 1996, 6616, 2024, 2017, 3331, 2055, 1029]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_tokens_to_ids(tokenize_and_cut('What the fuck are you talking about?'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "85ad2971",
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD = '[PAD]'\n",
    "CLS = '[CLS]'\n",
    "SEP = '[SEP]'\n",
    "UNK = '[UNK]'\n",
    "cls_token_id = 101\n",
    "sep_token_id = 102\n",
    "pad_token_id = 0\n",
    "unk_token_id = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a1442d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC = Field(use_vocab=False,\n",
    "                    tokenize=tokenize_and_cut,\n",
    "                    preprocessing=tokenizer.convert_tokens_to_ids,\n",
    "                    init_token=cls_token_id,\n",
    "                    eos_token=sep_token_id,\n",
    "                    pad_token=pad_token_id,\n",
    "                    unk_token=unk_token_id)\n",
    "\n",
    "TGT = Field(use_vocab=False,\n",
    "                    tokenize=tokenize_and_cut,\n",
    "                    preprocessing=tokenizer.convert_tokens_to_ids,\n",
    "                    init_token=cls_token_id,\n",
    "                    eos_token=sep_token_id,\n",
    "                    pad_token=pad_token_id,\n",
    "                    unk_token=unk_token_id)\n",
    "\n",
    "data_fields = [('src', SRC), ('tgt', TGT)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "520e04a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d8987aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/chatbotdata1.csv')\n",
    "src = df.q.values\n",
    "tgt = df.a.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d7af7394",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(src, tgt, test_size=0.4, random_state=666)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cb48cc76",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, X_dev, Y_test, Y_dev = train_test_split(X_test, Y_test, test_size=0.5, random_state=666)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "87079e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = {}\n",
    "test = {}\n",
    "dev = {}\n",
    "\n",
    "train['q'] = X_train\n",
    "train['a'] = Y_train\n",
    "\n",
    "test['q'] = X_test\n",
    "test['a'] = Y_test\n",
    "\n",
    "dev['q'] = X_dev\n",
    "dev['a'] = Y_dev\n",
    "\n",
    "train_df = pd.DataFrame(train)\n",
    "test_df = pd.DataFrame(test)\n",
    "dev_df = pd.DataFrame(dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cad3527c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_csv('./data/train.csv')\n",
    "test_df.to_csv('./data/test.csv')\n",
    "dev_df.to_csv('./data/dev.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6c19f3ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, validation_data, test_data = TabularDataset.splits(\n",
    "                    path = './data/',\n",
    "                    format='csv',\n",
    "                    train='train.csv',\n",
    "                    validation='dev.csv',\n",
    "                    test='test.csv',\n",
    "                    skip_header=False,\n",
    "                    fields= data_fields\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "12444c9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torchtext.data.dataset.TabularDataset at 0x1c7923e4a90>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "20a5866d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iterator, validation_iterator, test_iterator = BucketIterator.splits(\n",
    "    (train_data, validation_data, test_data),\n",
    "    batch_size=62,\n",
    "    sort_key=lambda x: len(x.src),  # function used to group the data\n",
    "    sort_within_batch=False,\n",
    "    device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f603a0fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "\n",
    "class TransBertEncoder(nn.Module):\n",
    "    def __init__(self, nhead=12, nlayers=12, dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        # bert encoder\n",
    "        self.bert = g_bert\n",
    "\n",
    "        # transformer encoder, as bert last layer fine-tune\n",
    "        self.pos_encoder = PositionalEncoding(g_bert_emb_dim, dropout)\n",
    "        encoder_layers = nn.TransformerEncoderLayer(d_model=g_bert_emb_dim, nhead=nhead)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, nlayers)\n",
    "\n",
    "    def forward(self, src):\n",
    "        # src = [src len, batch size]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # embedded = [src len, batch size, emb dim]\n",
    "            embedded = self.bert(src.transpose(0, 1))[0].transpose(0, 1)\n",
    "\n",
    "        # embedded = self.pos_encoder(embedded)\n",
    "\n",
    "        # src_mask = nn.Transformer().generate_square_subsequent_mask(len(embedded)).to(g_device)\n",
    "\n",
    "        # outputs = [src len, batch size, hid dim * n directions]\n",
    "        outputs = self.transformer_encoder(embedded)\n",
    "\n",
    "        return outputs\n",
    "\n",
    "\n",
    "class TransBertDecoder(nn.Module):\n",
    "    def __init__(self, nhead=8, nlayers=6, dropout=0.5):\n",
    "        super().__init__()\n",
    "\n",
    "        # bert encoder\n",
    "        self.bert = g_bert\n",
    "\n",
    "        self.pos_decoder = PositionalEncoding(g_bert_emb_dim, dropout)\n",
    "        decoder_layer = nn.TransformerDecoderLayer(d_model=g_bert_emb_dim, nhead=nhead)\n",
    "        self.transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers=nlayers)\n",
    "\n",
    "        self.fc_out = nn.Linear(g_bert_emb_dim, g_vocab_size)\n",
    "\n",
    "    def forward(self, tgt, meaning, teacher_forcing_ratio):\n",
    "        # tgt = [output_len, batch size]\n",
    "\n",
    "        output_len = tgt.size(0)\n",
    "        batch_size = tgt.size(1)\n",
    "        # decide if we are going to use teacher forcing or not\n",
    "        teacher_force = random.random() < teacher_forcing_ratio\n",
    "\n",
    "        if teacher_force and self.training:\n",
    "            tgt_emb_total = torch.zeros(output_len, batch_size, g_bert_emb_dim).to(g_device)\n",
    "\n",
    "            for t in range(0, output_len):\n",
    "                with torch.no_grad():\n",
    "                    tgt_emb = self.bert(tgt[:t+1].transpose(0, 1))[0].transpose(0, 1)\n",
    "                tgt_emb_total[t] = tgt_emb[-1]\n",
    "\n",
    "            tgt_mask = nn.Transformer().generate_square_subsequent_mask(len(tgt_emb_total)).to(g_device)\n",
    "            decoder_output = self.transformer_decoder(tgt=tgt_emb_total,\n",
    "                                                      memory=meaning,\n",
    "                                                      tgt_mask=tgt_mask)\n",
    "            predictions = self.fc_out(decoder_output)\n",
    "        else:\n",
    "            # initialized the input of the decoder with sos_idx (start of sentence token idx)\n",
    "            output = torch.full((output_len+1, batch_size), g_tokenizer.cls_token_id, dtype=torch.long, device=g_device)\n",
    "            predictions = torch.zeros(output_len, batch_size, g_vocab_size).to(g_device)\n",
    "\n",
    "            for t in range(0, output_len):\n",
    "                with torch.no_grad():\n",
    "                    tgt_emb = self.bert(output[:t+1].transpose(0, 1))[0].transpose(0, 1)\n",
    "\n",
    "                # tgt_emb = [t, batch size, emb dim]\n",
    "                # tgt_emb = self.pos_encoder(tgt_emb)\n",
    "\n",
    "                tgt_mask = nn.Transformer().generate_square_subsequent_mask(len(tgt_emb)).to(g_device)\n",
    "\n",
    "                # decoder_output = [t, batch size, emb dim]\n",
    "                decoder_output = self.transformer_decoder(tgt=tgt_emb,\n",
    "                                                          memory=meaning,\n",
    "                                                          tgt_mask=tgt_mask)\n",
    "\n",
    "                # prediction = [batch size, vocab size]\n",
    "                prediction = self.fc_out(decoder_output[-1])\n",
    "\n",
    "                # predictions = [output_len, batch size, vocab size]\n",
    "                predictions[t] = prediction\n",
    "\n",
    "                one_hot_idx = prediction.argmax(1)\n",
    "\n",
    "                # output  = [output len, batch size]\n",
    "                output[t+1] = one_hot_idx\n",
    "\n",
    "        return predictions\n",
    "\n",
    "\n",
    "class GruEncoder(nn.Module):\n",
    "    \"\"\"compress the request embeddings to meaning\"\"\"\n",
    "\n",
    "    def __init__(self, hidden_size, input_size):\n",
    "        super().__init__()\n",
    "        self.gru = nn.GRU(input_size, hidden_size)\n",
    "\n",
    "    def forward(self, input):\n",
    "        output, hidden = self.gru(input)\n",
    "        return hidden\n",
    "\n",
    "\n",
    "class GruDecoder(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        super().__init__()\n",
    "        self.gru = nn.GRU(output_size, hidden_size)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, src, tgt, hidden):\n",
    "        # first input to the decoder is the <CLS> tokens\n",
    "        fc_output = src[0].unsqueeze(0)\n",
    "        tgt_len = tgt.size(0)\n",
    "        batch_size = tgt.size(1)\n",
    "\n",
    "        # tensor to store decoder outputs\n",
    "        outputs = torch.zeros(tgt_len, batch_size, g_bert_emb_dim).to(g_device)\n",
    "\n",
    "        for t in range(0, tgt_len):\n",
    "            # insert input token embedding, previous hidden state and the context state\n",
    "            # receive output tensor (predictions) and new hidden state\n",
    "            gru_output, hidden = self.gru(fc_output, hidden)\n",
    "\n",
    "            fc_output = self.fc(gru_output)\n",
    "\n",
    "            # place predictions in a tensor holding predictions for each token\n",
    "            outputs[t] = fc_output\n",
    "        return outputs\n",
    "\n",
    "\n",
    "class DialogDNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, dropout=0.5):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, hidden_size)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, input):\n",
    "        # ResNet, dropout on first 3 layers\n",
    "        input = self.dropout(input)\n",
    "\n",
    "        output = input + F.relu(self.fc1(input))\n",
    "        output = self.dropout(output)\n",
    "\n",
    "        output = output + F.relu(self.fc2(output))\n",
    "        output = self.dropout(output)\n",
    "\n",
    "        output = output + self.fc3(output)  # no relu to keep negative values\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, transbert_encoder, transbert_decoder, gru_encoder, gru_decoder, dialog_dnn):\n",
    "        super().__init__()\n",
    "\n",
    "        self.transbert_encoder = transbert_encoder\n",
    "        self.transbert_decoder = transbert_decoder\n",
    "\n",
    "        self.gru_encoder = gru_encoder\n",
    "        self.gru_decoder = gru_decoder\n",
    "\n",
    "        self.dialog_dnn = dialog_dnn\n",
    "\n",
    "    def forward(self, src, tgt, teacher_forcing_ratio):\n",
    "        request_embeddings = self.transbert_encoder(src)\n",
    "        request_meaning = self.gru_encoder(request_embeddings)\n",
    "\n",
    "        if TRAIN_DIALOG:\n",
    "            response_meaning = self.dialog_dnn(request_meaning)\n",
    "        else:\n",
    "            response_meaning = request_meaning\n",
    "\n",
    "        response_embeddings = self.gru_decoder(request_embeddings, tgt, response_meaning)\n",
    "        response = self.transbert_decoder(tgt, response_embeddings, teacher_forcing_ratio)\n",
    "\n",
    "        return response"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
